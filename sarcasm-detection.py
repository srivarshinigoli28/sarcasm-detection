# -*- coding: utf-8 -*-
"""Copy of allinone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15eC7PJmJM7SoJE6sQPIF39cR5J-OpmPX
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle

# Load the dataset
data_df = pd.read_json('sarcasmdset.json', lines=True)

# Extract text and labels
texts = data_df['headline'].values
labels = data_df['is_sarcastic'].values

# Tokenization
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=100)

# Save the tokenizer for future use
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

# Splitting the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# Check shapes
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

model = keras.Sequential([
    keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=100),
    keras.layers.GlobalAveragePooling1D(),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))
print("Model Training Completed Successfully!")

model.save('sarcasm_model_initial.h5')
print("Model Saved Successfully!")

# Load the model
model = keras.models.load_model('sarcasm_model_initial.h5')

# Load the tokenizer
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Test Prediction
new_text = ["This is the best day of my life."]
new_seq = tokenizer.texts_to_sequences(new_text)
new_pad = pad_sequences(new_seq, maxlen=100)
prediction = model.predict(new_pad)

print(f"Prediction: {'Sarcastic' if prediction[0][0] > 0.5 else 'Not Sarcastic'}")

import tensorflow as tf
tf.config.run_functions_eagerly(True)

# Load the saved model
model = keras.models.load_model('sarcasm_model_initial.h5')

# Compile the model again (important to do this after loading)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Continue training the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))
print("Model Re-Training Completed Successfully!")

# Save the re-trained model
model.save('sarcasm_model_updated.h5')

# Save the retrained model
model.save('sarcasm_model_updated.h5')
print("Updated Model Saved Successfully!")

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Example prediction
new_texts = ["This is the best movie ever!", "Oh, great! Another Monday morning."]
new_sequences = tokenizer.texts_to_sequences(new_texts)
new_padded = pad_sequences(new_sequences, maxlen=100)

predictions = model.predict(new_padded)
for text, pred in zip(new_texts, predictions):
    print(f"'{text}' -> Sarcasm Probability: {pred[0]:.4f}")

# Function to predict if a message is sarcastic
max_length = 100
padding_type = 'post'
trunc_type = 'post'
def predict_sarcasm(message):
    sequence = tokenizer.texts_to_sequences([message])
    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    prediction = model.predict(padded_sequence)[0][0]
    if prediction > 0.5:
        return "Sarcastic"
    else:
        return "Not Sarcastic"

# Example usage
message = input("Enter a message to check if it's sarcastic: ")
result = predict_sarcasm(message)
print(f"Prediction: {result}")

